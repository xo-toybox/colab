{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xouyang1/project-notes/blob/main/colab_serve_vllm_cloudflared_tunnel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P2WqtsvUiXm",
        "outputId": "1ac880b6-9e03-4a33-e5b5-6db3ba3dd19d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: NVIDIA L4 (UUID: GPU-93683824-e52f-dcad-50b6-17ca2ba08203)\n",
            "Python 3.12.11\n",
            "python 3.12.11 cuda_ok True torch_cuda 12.6\n",
            "vLLM 0.10.1.1\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "nvidia-smi -L || true\n",
        "python -V\n",
        "pip -q install \"vllm>=0.6.1\" \"transformers>=4.44\" --upgrade\n",
        "\n",
        "python - <<'PY'\n",
        "import torch, sys\n",
        "print(\"python\", sys.version.split()[0],\n",
        "      \"cuda_ok\", torch.cuda.is_available(),\n",
        "      \"torch_cuda\", torch.version.cuda)\n",
        "try:\n",
        "    import vllm\n",
        "    print(\"vLLM\", vllm.__version__)\n",
        "except Exception as e:\n",
        "    print(\"vLLM import error:\", e)\n",
        "PY\n",
        "\n",
        "export VLLM_USE_RAY=0\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "export NCCL_P2P_DISABLE=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "aMPMsFlvU2ka"
      },
      "outputs": [],
      "source": [
        "MODEL=\"Qwen/Qwen3-0.6B\"\n",
        "PORT=8000\n",
        "\n",
        "# Useful memory flags for small/Colab GPUs:\n",
        "# --dtype float16 : fp16 weights\n",
        "# --gpu-memory-utilization 0.90 : let vLLM use up to ~90% VRAM\n",
        "# --max-model-len 4096 : cap contextb length (lower if you're tight on VRAM)\n",
        "# (If a model requires custom code, add: --trust-remote-code)\n",
        "!setsid nohup python -m vllm.entrypoints.openai.api_server \\\n",
        "    --model $MODEL \\\n",
        "    --host 0.0.0.0 --port $PORT \\\n",
        "    --dtype float16 \\\n",
        "    --gpu-memory-utilization 0.60 \\\n",
        "    --max-model-len 4096 \\\n",
        "    --download-dir /root/.cache/ \\\n",
        "    >server.log 2>&1 &\n",
        "!echo $! > server.pid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx66Lp_bW515",
        "outputId": "734ad29a-e450-4e92-af62-4c08572d912d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "python3 13268 root   33u  IPv4 289752      0t0  TCP *:8000 (LISTEN)\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.86it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.86it/s]\n",
            "\u001b[1;36m(EngineCore_0 pid=13488)\u001b[0;0m \n",
            "\u001b[1;36m(EngineCore_0 pid=13488)\u001b[0;0m INFO 09-06 05:21:13 [default_loader.py:262] Loading weights took 0.57 seconds\n",
            "\u001b[1;36m(EngineCore_0 pid=13488)\u001b[0;0m INFO 09-06 05:21:13 [gpu_model_runner.py:2007] Model loading took 1.1201 GiB and 1.720256 seconds\n",
            "\u001b[1;36m(EngineCore_0 pid=13488)\u001b[0;0m INFO 09-06 05:21:24 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/611660d274/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_0 pid=13488)\u001b[0;0m INFO 09-06 05:21:24 [backends.py:559] Dynamo bytecode transform time: 10.34 s\n",
            "\u001b[1;36m(EngineCore_0 pid=13488)\u001b[0;0m INFO 09-06 05:21:32 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.912 s\n",
            "\u001b[1;36m(EngineCore_0 pid=13488)\u001b[0;0m INFO 09-06 05:21:34 [monitor.py:34] torch.compile takes 10.34 s in total\n",
            "\u001b[1;36m(EngineCore_0 pid=13488)\u001b[0;0m INFO 09-06 05:21:35 [gpu_worker.py:276] Available KV cache memory: 10.77 GiB\n",
            "\u001b[1;36m(EngineCore_0 pid=13488)\u001b[0;0m INFO 09-06 05:21:36 [kv_cache_utils.py:849] GPU KV cache size: 100,832 tokens\n",
            "\u001b[1;36m(EngineCore_0 pid=13488)\u001b[0;0m INFO 09-06 05:21:36 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 24.62x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 25.82it/s]\n",
            "\u001b[1;36m(EngineCore_0 pid=13488)\u001b[0;0m INFO 09-06 05:21:39 [gpu_model_runner.py:2708] Graph capturing finished in 3 secs, took 0.47 GiB\n",
            "\u001b[1;36m(EngineCore_0 pid=13488)\u001b[0;0m INFO 09-06 05:21:39 [core.py:214] init engine (profile, create kv cache, warmup model) took 25.28 seconds\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:40 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 6302\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:40 [api_server.py:1611] Supported_tasks: ['generate']\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m WARNING 09-06 05:21:41 [__init__.py:1625] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [serving_responses.py:120] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [serving_chat.py:134] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [serving_completion.py:77] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [api_server.py:1880] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:36] Available routes are:\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /docs, Methods: GET, HEAD\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /redoc, Methods: GET, HEAD\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /health, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /load, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /ping, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /ping, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /tokenize, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /detokenize, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /v1/models, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /version, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /v1/responses, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /v1/chat/completions, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /v1/completions, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /v1/embeddings, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /pooling, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /classify, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /score, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /v1/score, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /v1/audio/translations, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /rerank, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /v1/rerank, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /v2/rerank, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /invocations, Methods: POST\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO 09-06 05:21:41 [launcher.py:44] Route: /metrics, Methods: GET\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO:     Started server process [13268]\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO:     Waiting for application startup.\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO:     Application startup complete.\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO:     127.0.0.1:38934 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO:     127.0.0.1:41704 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[1;36m(APIServer pid=13268)\u001b[0;0m INFO:     127.0.0.1:50400 - \"GET /v1/models HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "!lsof -i:8000 || ss -lntp | grep 8000 || true\n",
        "!curl -sS http://0.0.0.0:8000/v1/models | sed -n '1,120p'\n",
        "!tail -n 60 server.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyo4Z5SPXadS",
        "outputId": "b7a45f8c-f577-40cc-eb3f-4a1e425ff6ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x86_64\n",
            "cloudflared version 2025.8.1 (built 2025-08-21-1535 UTC)\n"
          ]
        }
      ],
      "source": [
        "# cloudflared PyPI package on Python 3.12 has import issues so manual download\n",
        "!uname -m  # should print x86_64 on Colab\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 /usr/local/bin/cloudflared\n",
        "!cloudflared --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWLvI8S9VdNE",
        "outputId": "db42fac2-dd63-4ac4-c72c-5b65a2bdc614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TUNNEL_URL=https://enclosed-oscar-cherry-postage.trycloudflare.com\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Launch tunnel in background and log output\n",
        "nohup cloudflared tunnel --url http://localhost:8000 --no-autoupdate \\\n",
        "  > /tmp/cloudflared.log 2>&1 &\n",
        "echo $! > /tmp/cloudflared.pid\n",
        "\n",
        "# Poll the log for the trycloudflare URL (up to ~30s)\n",
        "for i in {1..30}; do\n",
        "  URL=\"$(grep -o 'https://[a-z0-9-]*\\.trycloudflare\\.com' -m1 /tmp/cloudflared.log || true)\"\n",
        "  if [ -n \"$URL\" ]; then\n",
        "    echo \"TUNNEL_URL=$URL\"\n",
        "    exit 0\n",
        "  fi\n",
        "  sleep 1\n",
        "done\n",
        "\n",
        "echo \"Tunnel is starting... check logs: tail -f /tmp/cloudflared.log\"\n",
        "exit 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_wFO8jMYo99",
        "outputId": "8e1381b5-1a2b-45a6-b5b2-842825ad3121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "python3 13268 root   33u  IPv4 289752      0t0  TCP *:8000 (LISTEN)\n",
            "{\"object\":\"list\",\"data\":[{\"id\":\"Qwen/Qwen3-0.6B\",\"object\":\"model\",\"created\":1757136192,\"owned_by\":\"vllm\",\"root\":\"Qwen/Qwen3-0.6B\",\"parent\":null,\"max_model_len\":4096,\"permission\":[{\"id\":\"modelperm-fe76005467a349b1853b04aaf9f9d96f\",\"object\":\"model_permission\",\"created\":1757136192,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
          ]
        }
      ],
      "source": [
        "!lsof -i:8000\n",
        "!curl https://enclosed-oscar-cherry-postage.trycloudflare.com/v1/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP1bvplAVsBY",
        "outputId": "6ffbc24a-5363-4fa9-d657-80602454c971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-dfe763e3fed84483b864cf11e85d3e11\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1757136311,\n",
            "  \"model\": \"Qwen/Qwen3-0.6B\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"<think>\\nOkay, the user is asking for a one-sentence explanation of LoRA. Let me start by recalling what LoRA stands for. It's a method in the context of fine-tuning models for large language models, right? So, the key points here are that LoRA allows for small-scale modifications to the model parameters.\\n\\nWait, I should make sure not to mix up the terms. LoRA is a technique where the fine-tuning is done on a smaller dataset, not a large one. The model's parameters are modified using a small linear transformation. That's important because it makes the model more efficient without requiring massive\",\n",
            "        \"refusal\": null,\n",
            "        \"annotations\": null,\n",
            "        \"audio\": null,\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": [],\n",
            "        \"reasoning_content\": null\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"length\",\n",
            "      \"stop_reason\": null\n",
            "    }\n",
            "  ],\n",
            "  \"service_tier\": null,\n",
            "  \"system_fingerprint\": null,\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 16,\n",
            "    \"total_tokens\": 144,\n",
            "    \"completion_tokens\": 128,\n",
            "    \"prompt_tokens_details\": null\n",
            "  },\n",
            "  \"prompt_logprobs\": null,\n",
            "  \"kv_transfer_params\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "curl -sS https://enclosed-oscar-cherry-postage.trycloudflare.com/v1/chat/completions \\\n",
        "  -H 'Content-Type: application/json' \\\n",
        "  -d '{\n",
        "    \"model\":\"Qwen/Qwen3-0.6B\",\n",
        "    \"messages\":[{\"role\":\"user\",\"content\":\"Explain LoRA in one sentence.\"}],\n",
        "    \"max_tokens\":128,\n",
        "    \"temperature\":0.7\n",
        "  }' | jq ."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyP6kM5YAvcm8o3Je11cuSh6",
      "gpuType": "L4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
