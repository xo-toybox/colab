{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xouyang1/project-notes/blob/main/colab_serve_transformers_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgh4SuilOi8t",
        "outputId": "b246a1bc-fea5-4b36-996c-21d572cc83bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        }
      ],
      "source": [
        "!pip -q install \"gradio>=4.44.0,<5\" \"transformers>=4.44\" accelerate torch --upgrade\n",
        "\n",
        "import torch, gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
        "tok = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "def generate(prompt, max_new_tokens=128, temperature=0.7):\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=temperature)\n",
        "    return tok.decode(out[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "-dmQkKxhQDoR",
        "outputId": "63b75808-529a-4f0a-c4db-55411c8f29e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://876fc7910d8e5e8b6c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://876fc7910d8e5e8b6c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo = gr.Interface(\n",
        "    fn=generate,\n",
        "    inputs=[\"text\", gr.Slider(8, 512, value=128), gr.Slider(0.0, 1.5, value=0.7)],\n",
        "    outputs=\"text\",\n",
        "    title=\"LLM generate\",\n",
        "    api_name=\"predict\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDmxBIGIRRPy",
        "outputId": "d956658d-708b-4017-a1d0-41628048f6f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"data\":[\"Hello, I'm trying to create a function that takes a string and returns the number of vowels in it. Can you help me with that?\\nSure! Here's a function that can help you with that:\\n\\n```python\\ndef count_vowels(string):\\n    vowels = ['a', 'e', 'i', 'o', 'u']\\n    count = 0\\n    for char in string:\\n        if char in vowels:\\n            count += 1\\n    return count\\n```\\n\\nThis function takes a string as input and initializes a list of vowels. It then iterates over each character in the string and checks if it is\"],\"is_generating\":false,\"duration\":31.489562273025513,\"average_duration\":31.489562273025513,\"render_config\":null,\"changed_state_ids\":[]}"
          ]
        }
      ],
      "source": [
        "!curl -X POST https://876fc7910d8e5e8b6c.gradio.live/api/predict/ -H 'Content-Type: application/json' -d '{\"data\":[\"Hello\",128,0.7]}'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOLSjFXlG9hfdappr6qsi+T",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
